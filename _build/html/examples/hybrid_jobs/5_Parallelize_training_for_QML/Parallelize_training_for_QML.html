<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../../../genindex.html" /><link rel="search" title="Search" href="../../../search.html" />

    <!-- Generated with Sphinx 5.3.0 and Furo 2022.12.07 -->
        <title>Parallelize training for Quantum machine learning - Amazon Braket Examples documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo.css?digest=91d0f0d1c444bdcb17a68e833c7a53903343c195" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../../index.html"><div class="brand">Amazon Braket Examples  documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../../../index.html">
  
  
  <span class="sidebar-brand-text">Amazon Braket Examples  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  
</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="parallelize-training-for-quantum-machine-learning">
<h1>Parallelize training for Quantum machine learning<a class="headerlink" href="#parallelize-training-for-quantum-machine-learning" title="Permalink to this heading">#</a></h1>
<p>Quantum machine learning (QML) is a special type of hybrid quantum-classical workload. Like classical machine learning (ML), there is usually a parameterized model, a dataset and a loss function. When training the model with the dataset, the parameters of the model are updated to minimize the loss function. In QML, the model contains one or many quantum circuits. The model may or may not also include classical neural nets. A loss function is usually defined for a single data point. Say a dataset $D$ has $N_D$ data points, $d_1$, $d_2$, …, $d_{N_D}$. The losses associated with the data points are $L(d_1)$, $L(d_2)$, …, $L(d_{N_D})$, where $L$ is the loss function. Without invoking any advanced feature, the algorithm script would compute these losses in serial, and then average them to be the total loss for gradient computations. This procedure is time consuming, especially when there are hundreds of data points.</p>
<section id="data-parallelism">
<h2>Data parallelism<a class="headerlink" href="#data-parallelism" title="Permalink to this heading">#</a></h2>
<p>The loss from one data point is independent of the other data points. The order of the loss evaluations therefore does not need to follow a specific order. They can even be evaluated <i>in parallel!</i> Losses and gradients of variational parameters associated with different data points can be evaluated on different GPUs at the same time. This is known as data parallelism. In this notebook, we will learn to use <a class="reference external" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html">SageMaker’s distributed data parallel library</a> in Braket Jobs to accelerate the training of your quantum model. We go through examples to show you how to parallelize trainings across multiple GPUs in an instance, and even multiple GPUs over multiple instances!</p>
</section>
<section id="binary-classification-of-sonar-dataset">
<h2>Binary Classification of Sonar dataset<a class="headerlink" href="#binary-classification-of-sonar-dataset" title="Permalink to this heading">#</a></h2>
<p>Let’s use a binary classification of the <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+%28Sonar%2C+Mines+vs.+Rocks%29">Sonar dataset</a> as the QML example. The Sonar dataset contains 208 data points each with 60 features that are collected from sonar signals bouncing off materials. Each data point is either labeled as “M” for mines or “R” for rocks. Our QML model consists of an input layer, a quantum circuit and an output layer. The input and output layer are classical dense layers. The dimension of classical input layer is $60\times N$, where $N$ is the number of qubits in the quantum circuit. The result of the input layer is encoded into the quantum circuit using <a class="reference external" href="https://pennylane.readthedocs.io/en/stable/code/api/pennylane.AngleEmbedding.html">angle embedding</a>. After the angle embedding, the quantum circuit has the same structure as figure 4 of <a class="reference external" href="https://arxiv.org/abs/1804.00633">this paper</a>. It is a generic circuit ansatz that has two parametrized <a class="reference external" href="https://pennylane.readthedocs.io/en/user-docs-refactor/code/pennylane.templates.layers.StronglyEntanglingLayer.html">strongly entangling layers</a> and a single parametrized rotation gate at the first qubit. The measurement is only performed at the first qubit. Using the concept of classical ML, the dimension of the quantum circuit layer is $N\times1$. The classical output layer has dimension $1\times1$ which takes the measurement of the quantum circuit and outputs a real number. The loss function is the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html">margin loss function</a> between the model output and the label. See <span class="xref myst">model_def.py</span> and <span class="xref myst">quantum_circuit.py</span> for more detail about the model and the quantum circuits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_file_path</span> <span class="o">=</span> <span class="s2">&quot;data/sonar.all-data&quot;</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-with-single-gpu">
<h2>Training with single GPU<a class="headerlink" href="#training-with-single-gpu" title="Permalink to this heading">#</a></h2>
<p>Let’s start by running the job with <code class="docutils literal notranslate"><span class="pre">lightning.gpu</span></code> simulator on a single GPU in a <code class="docutils literal notranslate"><span class="pre">ml.p3.2xlarge</span></code> instance which has one Nvidia V100 GPU. The algorithm script to train our quantum model is <span class="xref myst">train_single.py</span>. In the algorithm script, we use PennyLane with PyTorch as our framework, which are both included in Braket’s pre-configured PyTorch container. As faster demonstration, we only use a portion of the dataset (64 data points) instead of the full dataset. You can experiment with more data by setting <code class="docutils literal notranslate"><span class="pre">ndata</span></code> in the hyperparameters to a higher number, up to 208, the size of dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">braket.jobs.config</span> <span class="kn">import</span> <span class="n">InstanceConfig</span>
<span class="kn">from</span> <span class="nn">braket.aws</span> <span class="kn">import</span> <span class="n">AwsSession</span>
<span class="kn">from</span> <span class="nn">braket.jobs.image_uris</span> <span class="kn">import</span> <span class="n">Framework</span><span class="p">,</span> <span class="n">retrieve_image</span>

<span class="n">instance_config</span> <span class="o">=</span> <span class="n">InstanceConfig</span><span class="p">(</span><span class="n">instanceType</span><span class="o">=</span><span class="s1">&#39;ml.p3.2xlarge&#39;</span><span class="p">)</span>

<span class="n">hyperparameters</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;nwires&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> 
                 <span class="s2">&quot;ndata&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> 
                 <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> 
                 <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> 
                 <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> 
                 <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
                <span class="p">}</span>

<span class="n">input_file_path</span> <span class="o">=</span> <span class="s2">&quot;data/sonar.all-data&quot;</span>

<span class="n">image_uri</span> <span class="o">=</span> <span class="n">retrieve_image</span><span class="p">(</span><span class="n">Framework</span><span class="o">.</span><span class="n">PL_PYTORCH</span><span class="p">,</span> <span class="n">AwsSession</span><span class="p">()</span><span class="o">.</span><span class="n">region</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We submit our job after setting up instance configuration, hyperparameters and the job container image.</p>
<p><strong>Note:</strong> The following cell may be unable to complete with the default resource limits. You may contact <a class="reference external" href="https://support.console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase">AWS Support</a> to increase the limits on your account.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">braket.aws</span> <span class="kn">import</span> <span class="n">AwsQuantumJob</span>

<span class="n">job</span> <span class="o">=</span> <span class="n">AwsQuantumJob</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;local:pennylane/lightning.gpu&quot;</span><span class="p">,</span>
    <span class="n">source_module</span><span class="o">=</span><span class="s2">&quot;qml_script&quot;</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s2">&quot;qml_script.train_single&quot;</span><span class="p">,</span>
    <span class="n">job_name</span><span class="o">=</span><span class="s2">&quot;qml-single-&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())),</span>
    <span class="n">hyperparameters</span><span class="o">=</span><span class="n">hyperparameters</span><span class="p">,</span>
    <span class="n">input_data</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input-data&quot;</span><span class="p">:</span> <span class="n">input_file_path</span><span class="p">},</span>
    <span class="n">instance_config</span><span class="o">=</span><span class="n">instance_config</span><span class="p">,</span>
    <span class="n">image_uri</span><span class="o">=</span><span class="n">image_uri</span><span class="p">,</span>
    <span class="n">wait_until_complete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell should take about 7 minutes</span>
<span class="nb">print</span><span class="p">(</span><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;last loss&#39;: 0.09291739761829376}
</pre></div>
</div>
</div>
</div>
</section>
<section id="modify-your-algorithm-script-for-data-parallelism-a-class-anchor-id-modify-a">
<h2>Modify your algorithm script for data parallelism <a class="anchor" id="modify"></a><a class="headerlink" href="#modify-your-algorithm-script-for-data-parallelism-a-class-anchor-id-modify-a" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch</a> has built-in features for data parallelism. With SageMaker’s distributed data parallel library, Braket Jobs makes it easier for you to leverage data parallelism to accelerate your training. To use data parallelism, you need to slightly modify your algorithm script. As an example, we modify the algorithm script <span class="xref myst">train_single.py</span> to <span class="xref myst">train_dp.py</span>. Let’s go through the changes.</p>
<p>First, we import the <code class="docutils literal notranslate"><span class="pre">smdistributed</span></code> package which does most of the heavy lifting for distributing your workloads across multiple GPUs and/or multiple instances. This package is pre-configured in the Braket PyTorch and TensorFlow containers. The <code class="docutils literal notranslate"><span class="pre">DDP</span></code> class from <code class="docutils literal notranslate"><span class="pre">smdistributed</span></code> converts the quantum model into a data parallelizable model. The <code class="docutils literal notranslate"><span class="pre">dist</span></code> module  tell our algorithm script the total number of GPUs for the training (<code class="docutils literal notranslate"><span class="pre">world_size</span></code>), and the <code class="docutils literal notranslate"><span class="pre">rank</span></code> and <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> of a GPU. <code class="docutils literal notranslate"><span class="pre">rank</span></code> is the absolute index of a GPU across all instances, while <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> is the index of a GPU within an instance. For example, if there are four instances each with eight GPUs allocated for the training, the <code class="docutils literal notranslate"><span class="pre">rank</span></code> ranges from 0 to 31 and the <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> ranges from 0 to 7.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>sed -n <span class="m">22</span>,23p qml_script/train_dp.py
<span class="o">!</span>sed -n <span class="m">52</span>,60p qml_script/train_dp.py
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>import smdistributed.dataparallel.torch.distributed as dist
from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP
    dp_info = {
        &quot;world_size&quot;: dist.get_world_size(),
        &quot;rank&quot;: dist.get_rank(),
        &quot;local_rank&quot;: dist.get_local_rank(),
    }
    batch_size //= dp_info[&quot;world_size&quot;] // 8
    batch_size = max(batch_size, 1)
    print(&quot;dp_info: &quot;, dp_info)
</pre></div>
</div>
</div>
</div>
<p>Next, we define a <code class="docutils literal notranslate"><span class="pre">DistributedSampler</span></code> according to the <code class="docutils literal notranslate"><span class="pre">world_size</span></code> and <code class="docutils literal notranslate"><span class="pre">rank</span></code>, and pass it into the data loader. This sampler avoids GPUs accessing the same slice of a dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>sed -n <span class="m">67</span>,79p qml_script/train_dp.py
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset, 
        num_replicas=dp_info[&quot;world_size&quot;], 
        rank=dp_info[&quot;rank&quot;]
    )
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        sampler=train_sampler,        
    )
</pre></div>
</div>
</div>
</div>
<p>Next, we use the <code class="docutils literal notranslate"><span class="pre">DDP</span></code> class to make our quantum model parallelizable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>sed -n <span class="m">91</span>,94p qml_script/train_dp.py
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    model = DressedQNN(qc_dev).to(device)
    model = DDP(model)
    torch.cuda.set_device(dp_info[&quot;local_rank&quot;])
    model.cuda(dp_info[&quot;local_rank&quot;])
</pre></div>
</div>
</div>
</div>
<p>The above are the programming change you need to make to use data parallelism. In QML, we often want to save models, save results and print training progress. If each GPU executes the saving and printing command, the log would be flooded with the repeated information, and the model and results would be overwriting each other. To avoid this, we only save and print from the GPU that has <code class="docutils literal notranslate"><span class="pre">rank</span></code> 0.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>sed -n <span class="m">118</span>,121p qml_script/train_dp.py
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    if dp_info[&quot;rank&quot;] == 0:    
        print(&quot;Training Finished!!&quot;)
        torch.save(model.state_dict(), f&quot;{output_dir}/test_local.pt&quot;)
        save_job_result({&quot;last loss&quot;: float(loss_before.detach().cpu())})
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-with-multiple-gpus-in-single-instance">
<h2>Training with multiple GPUs in single instance<a class="headerlink" href="#training-with-multiple-gpus-in-single-instance" title="Permalink to this heading">#</a></h2>
<p>With the modified algorithm, we can now submit our job with data parallelism. Amazon Braket Hybrid Jobs supports <code class="docutils literal notranslate"><span class="pre">ml.p3.16xlarge</span></code> for SageMaker distributed data parallel library. Be sure to choose this instance type from the list and configure it through the <code class="docutils literal notranslate"><span class="pre">InstanceConfig</span></code> argument in Jobs. See this <a class="reference external" href="https://aws.amazon.com/ec2/instance-types/?tag=local002-20">documentation</a> for the specification, and see the <a class="reference external" href="https://aws.amazon.com/braket/pricing/?tag=local002-20">Amazon Braket pricing page</a> for the cost of the instance type.</p>
<p>For the SageMaker distributed data parallel library to know that data parallelism is enabled, we set the <code class="docutils literal notranslate"><span class="pre">distribution</span></code> argument to be <code class="docutils literal notranslate"><span class="pre">&quot;data_parallel&quot;</span></code> when creating a job. This argument triggers Braket Jobs to add two additional hyperparameters, <code class="docutils literal notranslate"><span class="pre">&quot;sagemaker_distributed_dataparallel_enabled&quot;</span></code> setting to <code class="docutils literal notranslate"><span class="pre">&quot;true&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;sagemaker_instance_type&quot;</span></code> setting to the instance type we are using. These two hyperparameters are used by the <code class="docutils literal notranslate"><span class="pre">smdistributed</span></code> package at runtime. You do not need to explicitly call them in the algorithm script. Keep in mind that data parallelism only works correctly when you modify your algorithm script according to the <span class="xref myst">previous section</span>. If the data parallelism option is enabled in the hyperparameters without a correctly modified algorithm script, the job may throw errors, or each GPU may repeat the same workload without data parallelism.</p>
<p><i>Warning: The p3.16xlarge instance has higher cost per minute. The cell below may incur charge up to $4. Run this cell only if you are comfortable with the charge. </i></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">braket.jobs.config</span> <span class="kn">import</span> <span class="n">InstanceConfig</span>
<span class="kn">from</span> <span class="nn">braket.aws</span> <span class="kn">import</span> <span class="n">AwsSession</span>
<span class="kn">from</span> <span class="nn">braket.jobs.image_uris</span> <span class="kn">import</span> <span class="n">Framework</span><span class="p">,</span> <span class="n">retrieve_image</span>

<span class="n">instance_config</span> <span class="o">=</span> <span class="n">InstanceConfig</span><span class="p">(</span><span class="n">instanceType</span><span class="o">=</span><span class="s1">&#39;ml.p3.16xlarge&#39;</span><span class="p">)</span>

<span class="n">hyperparameters</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;nwires&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> 
                 <span class="s2">&quot;ndata&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> 
                 <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> 
                 <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> 
                 <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> 
                 <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
                <span class="p">}</span>

<span class="n">input_file_path</span> <span class="o">=</span> <span class="s2">&quot;data/sonar.all-data&quot;</span>

<span class="n">image_uri</span> <span class="o">=</span> <span class="n">retrieve_image</span><span class="p">(</span><span class="n">Framework</span><span class="o">.</span><span class="n">PL_PYTORCH</span><span class="p">,</span> <span class="n">AwsSession</span><span class="p">()</span><span class="o">.</span><span class="n">region</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With the instance type and data parallelism configured, we can now submit our job. There are 8 GPUs in a <code class="docutils literal notranslate"><span class="pre">ml.p3.16xlarge</span></code> instance. When the instance spins up, the workload is distributed across the 8 GPUs.</p>
<p><strong>Note:</strong> The following cell may be unable to complete with the default resource limits. You may contact <a class="reference external" href="https://support.console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase">AWS Support</a> to increase the limits on your account.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">braket.aws</span> <span class="kn">import</span> <span class="n">AwsQuantumJob</span>

<span class="n">job</span> <span class="o">=</span> <span class="n">AwsQuantumJob</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;local:pennylane/lightning.gpu&quot;</span><span class="p">,</span>
    <span class="n">source_module</span><span class="o">=</span><span class="s2">&quot;qml_script&quot;</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s2">&quot;qml_script.train_dp&quot;</span><span class="p">,</span>
    <span class="n">job_name</span><span class="o">=</span><span class="s2">&quot;qml-dp1x-&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())),</span>
    <span class="n">hyperparameters</span><span class="o">=</span><span class="n">hyperparameters</span><span class="p">,</span>
    <span class="n">input_data</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input-data&quot;</span><span class="p">:</span> <span class="n">input_file_path</span><span class="p">},</span>
    <span class="n">instance_config</span><span class="o">=</span><span class="n">instance_config</span><span class="p">,</span>
    <span class="n">distribution</span><span class="o">=</span><span class="s2">&quot;data_parallel&quot;</span><span class="p">,</span>
    <span class="n">image_uri</span><span class="o">=</span><span class="n">image_uri</span><span class="p">,</span>
    <span class="n">wait_until_complete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell should take about 7 minutes</span>
<span class="nb">print</span><span class="p">(</span><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;last loss&#39;: 0.1675658121705055}
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-with-multiple-gpus-across-multiple-instances">
<h2>Training with multiple GPUs across multiple instances<a class="headerlink" href="#training-with-multiple-gpus-across-multiple-instances" title="Permalink to this heading">#</a></h2>
<p>We are not limited to parallelizing the workload inside a single instance. We can perform distributed training by parallelizing our workload across multiple instances. With the algorithm script modified and the data parallelism enabled, we can perform distributed data parallelism by setting instance count larger than 1. To configure instance count, we use the <code class="docutils literal notranslate"><span class="pre">instanceCount</span></code> argument in <code class="docutils literal notranslate"><span class="pre">InstanceConfig</span></code>. The SageMaker distributed library we included in our algorithm script coordinates the multiple instances and conducts the distributed training for us.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">instance_config</span> <span class="o">=</span> <span class="n">InstanceConfig</span><span class="p">(</span><span class="n">instanceType</span><span class="o">=</span><span class="s1">&#39;ml.p3.16xlarge&#39;</span><span class="p">,</span> <span class="n">instanceCount</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Be mindful that, when using multiple instances, each instance incurs charge based on how long you use it. In distributed data parallelism, when you set <code class="docutils literal notranslate"><span class="pre">instanceCount=2</span></code>, two instances are allocated to run your job. SageMaker distributed library managed the instances that they start and end at the same time. If your workload takes 200 seconds, you will be billed for 200 seconds for each instance used, which adds to 400 seconds in total.</p>
<p><i> Warning: The p3.16xlarge instance has higher cost per minute. The cell below may incur charge up to $8. Run this cell only if you are comfortable with the charge. </i></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">braket.jobs.config</span> <span class="kn">import</span> <span class="n">InstanceConfig</span>
<span class="kn">from</span> <span class="nn">braket.aws</span> <span class="kn">import</span> <span class="n">AwsSession</span>
<span class="kn">from</span> <span class="nn">braket.jobs.image_uris</span> <span class="kn">import</span> <span class="n">Framework</span><span class="p">,</span> <span class="n">retrieve_image</span>

<span class="n">hyperparameters</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;nwires&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> 
                 <span class="s2">&quot;ndata&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> 
                 <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">64</span><span class="p">,</span> 
                 <span class="s2">&quot;epochs&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> 
                 <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> 
                 <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
                 <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
                <span class="p">}</span>

<span class="n">input_file_path</span> <span class="o">=</span> <span class="s2">&quot;data/sonar.all-data&quot;</span>

<span class="n">image_uri</span> <span class="o">=</span> <span class="n">retrieve_image</span><span class="p">(</span><span class="n">Framework</span><span class="o">.</span><span class="n">PL_PYTORCH</span><span class="p">,</span> <span class="n">AwsSession</span><span class="p">()</span><span class="o">.</span><span class="n">region</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can submit our job with distributed data parallelism!</p>
<p><strong>Note:</strong> The following cell may be unable to complete with the default resource limits. You may contact <a class="reference external" href="https://support.console.aws.amazon.com/support/home#/case/create?issueType=service-limit-increase">AWS Support</a> to increase the limits on your account.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">braket.aws</span> <span class="kn">import</span> <span class="n">AwsQuantumJob</span>

<span class="n">job</span> <span class="o">=</span> <span class="n">AwsQuantumJob</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;local:pennylane/lightning.gpu&quot;</span><span class="p">,</span>
    <span class="n">source_module</span><span class="o">=</span><span class="s2">&quot;qml_script&quot;</span><span class="p">,</span>
    <span class="n">entry_point</span><span class="o">=</span><span class="s2">&quot;qml_script.train_dp&quot;</span><span class="p">,</span>
    <span class="n">job_name</span><span class="o">=</span><span class="s2">&quot;qml-dp2x-&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">())),</span>
    <span class="n">hyperparameters</span><span class="o">=</span><span class="n">hyperparameters</span><span class="p">,</span>
    <span class="n">input_data</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;input-data&quot;</span><span class="p">:</span> <span class="n">input_file_path</span><span class="p">},</span>
    <span class="n">instance_config</span><span class="o">=</span><span class="n">instance_config</span><span class="p">,</span>
    <span class="n">distribution</span><span class="o">=</span><span class="s2">&quot;data_parallel&quot;</span><span class="p">,</span>
    <span class="n">image_uri</span><span class="o">=</span><span class="n">image_uri</span><span class="p">,</span>
    <span class="n">wait_until_complete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell should take about 7 minutes</span>
<span class="nb">print</span><span class="p">(</span><span class="n">job</span><span class="o">.</span><span class="n">result</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;last loss&#39;: 0.26321490332484243}
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>In this notebook, we show you how to use SageMaker distributed library to parallelize quantum machine learning workloads. To learn more about distributed training, you can read the <a class="reference external" href="https://docs.aws.amazon.com/braket/latest/developerguide/braket-jobs.html">Amazon Braket documentation</a> and <a class="reference external" href="https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html?tag=local002-20">Amazon SageMaker Distributed Training Libraries</a>.</p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Parallelize training for Quantum machine learning</a><ul>
<li><a class="reference internal" href="#data-parallelism">Data parallelism</a></li>
<li><a class="reference internal" href="#binary-classification-of-sonar-dataset">Binary Classification of Sonar dataset</a></li>
<li><a class="reference internal" href="#training-with-single-gpu">Training with single GPU</a></li>
<li><a class="reference internal" href="#modify-your-algorithm-script-for-data-parallelism-a-class-anchor-id-modify-a">Modify your algorithm script for data parallelism <a class="anchor" id="modify"></a></a></li>
<li><a class="reference internal" href="#training-with-multiple-gpus-in-single-instance">Training with multiple GPUs in single instance</a></li>
<li><a class="reference internal" href="#training-with-multiple-gpus-across-multiple-instances">Training with multiple GPUs across multiple instances</a></li>
<li><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/scripts/furo.js"></script>
    </body>
</html>